{"created": 1757483753.0863118, "duration": 2.6122848987579346, "exitcode": 1, "root": "/Users/ilpinto/dev/AlchimistProject/alc-lite", "environment": {}, "summary": {"passed": 1, "failed": 3, "total": 4, "collected": 4}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/test_global_execution_lock.py::TestGlobalLockPerformance::test_lock_acquisition_speed", "type": "Coroutine", "lineno": 402}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_complete_pause_resume_flow", "type": "Coroutine", "lineno": 46}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_pause_resume_on_failure", "type": "Coroutine", "lineno": 135}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_concurrent_execution_prevention_with_pause", "type": "Coroutine", "lineno": 221}]}], "tests": [{"nodeid": "tests/test_global_execution_lock.py::TestGlobalLockPerformance::test_lock_acquisition_speed", "lineno": 402, "outcome": "passed", "keywords": ["test_lock_acquisition_speed", "asyncio", "pytestmark", "TestGlobalLockPerformance", "performance", "test_global_execution_lock.py", "tests", "alc-lite", ""], "setup": {"duration": 1.2961641669971868, "outcome": "passed"}, "call": {"duration": 0.0018241250654682517, "outcome": "passed"}, "teardown": {"duration": 0.0009023750899359584, "outcome": "passed"}}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_complete_pause_resume_flow", "lineno": 46, "outcome": "failed", "keywords": ["test_complete_pause_resume_flow", "asyncio", "pytestmark", "TestPauseResumeE2E", "test_pause_resume_e2e.py", "tests", "alc-lite", ""], "setup": {"duration": 0.0022600420052185655, "outcome": "passed"}, "call": {"duration": 0.0003547499654814601, "outcome": "failed", "crash": {"path": "/Users/ilpinto/dev/AlchimistProject/alc-lite/tests/test_pause_resume_e2e.py", "lineno": 50, "message": "TypeError: object dict can't be used in 'await' expression"}, "traceback": [{"path": "tests/test_pause_resume_e2e.py", "lineno": 50, "message": "TypeError"}], "longrepr": "self = <tests.test_pause_resume_e2e.TestPauseResumeE2E object at 0x11ac16980>\nsetup_e2e_environment = {'executor': <modules.Arbitrage.sfr.parallel_executor.ParallelLegExecutor object at 0x168029d80>, 'lock': <modules.Arb... 'mock_ib': <MagicMock id='6039937152'>, 'strategy': <modules.Arbitrage.Strategy.ArbitrageClass object at 0x168010e50>}\n\n    @pytest.mark.asyncio\n    async def test_complete_pause_resume_flow(self, setup_e2e_environment):\n        \"\"\"Test the complete pause/resume flow during parallel execution\"\"\"\n>       env = await setup_e2e_environment\nE       TypeError: object dict can't be used in 'await' expression\n\ntests/test_pause_resume_e2e.py:50: TypeError"}, "teardown": {"duration": 0.0003739160019904375, "outcome": "passed"}}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_pause_resume_on_failure", "lineno": 135, "outcome": "failed", "keywords": ["test_pause_resume_on_failure", "asyncio", "pytestmark", "TestPauseResumeE2E", "test_pause_resume_e2e.py", "tests", "alc-lite", ""], "setup": {"duration": 0.0015946249477565289, "outcome": "passed"}, "call": {"duration": 0.0002749580889940262, "outcome": "failed", "crash": {"path": "/Users/ilpinto/dev/AlchimistProject/alc-lite/tests/test_pause_resume_e2e.py", "lineno": 139, "message": "TypeError: object dict can't be used in 'await' expression"}, "traceback": [{"path": "tests/test_pause_resume_e2e.py", "lineno": 139, "message": "TypeError"}], "longrepr": "self = <tests.test_pause_resume_e2e.TestPauseResumeE2E object at 0x11ac15ab0>\nsetup_e2e_environment = {'executor': <modules.Arbitrage.sfr.parallel_executor.ParallelLegExecutor object at 0x1680a79d0>, 'lock': <modules.Arb... 'mock_ib': <MagicMock id='6039973232'>, 'strategy': <modules.Arbitrage.Strategy.ArbitrageClass object at 0x16807be20>}\n\n    @pytest.mark.asyncio\n    async def test_pause_resume_on_failure(self, setup_e2e_environment):\n        \"\"\"Test pause/resume flow when execution fails\"\"\"\n>       env = await setup_e2e_environment\nE       TypeError: object dict can't be used in 'await' expression\n\ntests/test_pause_resume_e2e.py:139: TypeError"}, "teardown": {"duration": 0.0005114160012453794, "outcome": "passed"}}, {"nodeid": "tests/test_pause_resume_e2e.py::TestPauseResumeE2E::test_concurrent_execution_prevention_with_pause", "lineno": 221, "outcome": "failed", "keywords": ["test_concurrent_execution_prevention_with_pause", "asyncio", "pytestmark", "TestPauseResumeE2E", "test_pause_resume_e2e.py", "tests", "alc-lite", ""], "setup": {"duration": 0.0007067909464240074, "outcome": "passed"}, "call": {"duration": 0.0008163750171661377, "outcome": "failed", "crash": {"path": "/Users/ilpinto/micromamba/lib/python3.10/unittest/mock.py", "lineno": 1420, "message": "AttributeError: None does not have the attribute 'create_execution_plan'"}, "traceback": [{"path": "tests/test_pause_resume_e2e.py", "lineno": 246, "message": ""}, {"path": "../../../micromamba/lib/python3.10/unittest/mock.py", "lineno": 1447, "message": ""}, {"path": "../../../micromamba/lib/python3.10/unittest/mock.py", "lineno": 1420, "message": "AttributeError"}], "longrepr": "self = <tests.test_pause_resume_e2e.TestPauseResumeE2E object at 0x11ac157e0>\n\n    @pytest.mark.asyncio\n    async def test_concurrent_execution_prevention_with_pause(self):\n        \"\"\"Test that global lock + pause prevents concurrent executions\"\"\"\n        strategy = ArbitrageClass()\n    \n        # Create two executors for different symbols\n        mock_ib = MagicMock()\n        mock_ib.isConnected.return_value = True\n    \n        executor1 = ParallelLegExecutor(ib=mock_ib, symbol=\"SPY\", strategy=strategy)\n        executor2 = ParallelLegExecutor(ib=mock_ib, symbol=\"QQQ\", strategy=strategy)\n    \n        # Track which executor gets to pause\n        pause_calls = []\n    \n        async def track_pause(symbol):\n            pause_calls.append(symbol)\n            await strategy._ArbitrageClass__class__.pause_all_other_executors(\n                strategy, symbol\n            )\n    \n        strategy.pause_all_other_executors = track_pause\n    \n        # Mock execution for both\n>       with patch.object(executor1.framework, \"create_execution_plan\"):\n\ntests/test_pause_resume_e2e.py:246: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x1680bd6f0>\n\n    def __enter__(self):\n        \"\"\"Perform the patch.\"\"\"\n        new, spec, spec_set = self.new, self.spec, self.spec_set\n        autospec, kwargs = self.autospec, self.kwargs\n        new_callable = self.new_callable\n        self.target = self.getter()\n    \n        # normalise False to None\n        if spec is False:\n            spec = None\n        if spec_set is False:\n            spec_set = None\n        if autospec is False:\n            autospec = None\n    \n        if spec is not None and autospec is not None:\n            raise TypeError(\"Can't specify spec and autospec\")\n        if ((spec is not None or autospec is not None) and\n            spec_set not in (True, None)):\n            raise TypeError(\"Can't provide explicit spec_set *and* spec or autospec\")\n    \n>       original, local = self.get_original()\n\n../../../micromamba/lib/python3.10/unittest/mock.py:1447: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <unittest.mock._patch object at 0x1680bd6f0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: None does not have the attribute 'create_execution_plan'\n\n../../../micromamba/lib/python3.10/unittest/mock.py:1420: AttributeError"}, "teardown": {"duration": 0.0004628329770639539, "outcome": "passed"}}]}
