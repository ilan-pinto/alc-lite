name: CI/CD Pipeline

on:
  push:
    branches:
      - master
      - dev
      - 'opt-**'
      - 'fix-**'
      - 'feat-**'
  pull_request:
    branches: [ master, dev ]
  release:
    types: [ published ]

jobs:
  ci:
    runs-on: macos-latest

    strategy:
      matrix:
        python-runtime: ['cpython', 'pypy']
        include:
          - python-runtime: 'cpython'
            python-version: '3.10'
            cache-key: 'cpython'
          - python-runtime: 'pypy'
            python-version: 'pypy3.10'
            cache-key: 'pypy'

    name: CI (${{ matrix.python-runtime }})


    permissions:
      contents: write
      actions: read
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python (${{ matrix.python-runtime }})
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.cache-key }}-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.cache-key }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip

        # Install PyPy-compatible dependencies or regular dependencies
        if [ "${{ matrix.python-runtime }}" = "pypy" ]; then
          if [ -f "requirements-pypy.txt" ]; then
            pip install -r requirements-pypy.txt
          else
            pip install -r requirements.txt
          fi
        else
          pip install -r requirements.txt
        fi

        # Install development dependencies (some may not work with PyPy)
        pip install flake8 black isort || echo "Some dev dependencies failed to install"

        # Skip mypy on PyPy as it's not compatible
        if [ "${{ matrix.python-runtime }}" = "cpython" ]; then
          pip install mypy
        fi

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --statistics

    - name: Format check with black
      run: |
        black --check --diff .

    - name: Import sorting check with isort
      run: |
        isort --check-only --diff .

    # - name: Type checking with mypy
    #   run: |
    #     mypy . --ignore-missing-imports

    - name: Run tests with pytest
      run: |
        # Use appropriate Python executable based on runtime
        if [ "${{ matrix.python-runtime }}" = "pypy" ]; then
          PYTHON_CMD="pypy3"
        else
          PYTHON_CMD="python"
        fi

        $PYTHON_CMD -m pytest tests/ -v --cov=commands --cov=modules --cov-report=term-missing --cov-report=xml --cov-report=html --cov-fail-under=20

    - name: Upload coverage reports to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Get version information
      id: version
      run: |
        # Extract version from Python files or use git tag
        if [ -f "alchimest.py" ]; then
          VERSION=$(grep -o '__version__ = "[^"]*"' alchimest.py | cut -d'"' -f2 || echo "0.1.0")
        else
          VERSION="0.1.0"
        fi

        # Get git information
        GIT_HASH=$(git rev-parse --short HEAD)
        GIT_TAG=$(git describe --tags --exact-match 2>/dev/null || echo "dev")

        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "git_hash=$GIT_HASH" >> $GITHUB_OUTPUT
        echo "git_tag=$GIT_TAG" >> $GITHUB_OUTPUT

        echo "Version: $VERSION"
        echo "Git Hash: $GIT_HASH"
        echo "Git Tag: $GIT_TAG"

    - name: Generate changelog
      id: changelog
      if: github.event_name == 'push' && github.ref == 'refs/heads/master'
      run: |
        echo "🔄 Generating changelog..."

        # Get the latest tag or use initial version
        PREVIOUS_VERSION=$(git describe --tags --abbrev=0 2>/dev/null || echo "0.0.0")

        CURRENT_VERSION="${{ steps.version.outputs.version }}"

        echo "Previous version: $PREVIOUS_VERSION"
        echo "Current version: $CURRENT_VERSION"

        # Generate changelog from git commits
        CHANGELOG_FILE="CHANGELOG_TEMP.md"

        echo "### 📝 Changelog" > $CHANGELOG_FILE
        echo "" >> $CHANGELOG_FILE

        # Check if we have any previous tags to compare against
        if git tag -l | grep -q "v$PREVIOUS_VERSION"; then
          COMMIT_RANGE="v$PREVIOUS_VERSION..HEAD"
          echo "**Changes since v$PREVIOUS_VERSION:**" >> $CHANGELOG_FILE
        else
          # If no previous tag exists, get all commits
          COMMIT_RANGE="HEAD"
          echo "**All changes in this version:**" >> $CHANGELOG_FILE
        fi

        echo "" >> $CHANGELOG_FILE

        # Get commits with categorization
        FEAT_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^feat" --grep="^feature" || true)
        FIX_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^fix" --grep="^bugfix" || true)
        REFACTOR_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^refactor" --grep="^refact" || true)
        DOCS_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^docs" --grep="^doc" || true)
        TEST_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^test" --grep="^tests" || true)
        CHORE_COMMITS=$(git log $COMMIT_RANGE --oneline --grep="^chore" --grep="^ci" --grep="^build" || true)

        # Get all commits for "Other Changes" section
        ALL_COMMITS=$(git log $COMMIT_RANGE --oneline --pretty=format:"%h %s" || true)

        # Function to format commits
        format_commits() {
          local commits="$1"
          if [ -n "$commits" ]; then
            echo "$commits" | while IFS= read -r line; do
              if [ -n "$line" ]; then
                echo "- $line"
              fi
            done
          fi
        }

        # Add categorized commits to changelog
        if [ -n "$FEAT_COMMITS" ]; then
          echo "#### 🚀 New Features" >> $CHANGELOG_FILE
          format_commits "$FEAT_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        if [ -n "$FIX_COMMITS" ]; then
          echo "#### 🐛 Bug Fixes" >> $CHANGELOG_FILE
          format_commits "$FIX_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        if [ -n "$REFACTOR_COMMITS" ]; then
          echo "#### ♻️ Code Refactoring" >> $CHANGELOG_FILE
          format_commits "$REFACTOR_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        if [ -n "$DOCS_COMMITS" ]; then
          echo "#### 📚 Documentation" >> $CHANGELOG_FILE
          format_commits "$DOCS_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        if [ -n "$TEST_COMMITS" ]; then
          echo "#### 🧪 Tests" >> $CHANGELOG_FILE
          format_commits "$TEST_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        if [ -n "$CHORE_COMMITS" ]; then
          echo "#### 🔧 Chores & CI" >> $CHANGELOG_FILE
          format_commits "$CHORE_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        # Add all commits as "Other Changes" if they don't fit above categories
        if [ -n "$ALL_COMMITS" ]; then
          echo "#### 📋 All Changes" >> $CHANGELOG_FILE
          format_commits "$ALL_COMMITS" >> $CHANGELOG_FILE
          echo "" >> $CHANGELOG_FILE
        fi

        # Add commit statistics
        COMMIT_COUNT=$(git rev-list --count $COMMIT_RANGE 2>/dev/null || echo "0")
        CONTRIBUTORS=$(git log $COMMIT_RANGE --format="%an" | sort -u | wc -l 2>/dev/null || echo "1")

        echo "#### 📊 Statistics" >> $CHANGELOG_FILE
        echo "- **Commits:** $COMMIT_COUNT" >> $CHANGELOG_FILE
        echo "- **Contributors:** $CONTRIBUTORS" >> $CHANGELOG_FILE
        echo "- **Files changed:** $(git diff --name-only $COMMIT_RANGE | wc -l 2>/dev/null || echo "N/A")" >> $CHANGELOG_FILE

        # Store changelog content for use in release
        CHANGELOG_CONTENT=$(cat $CHANGELOG_FILE)
        echo "changelog<<EOF" >> $GITHUB_OUTPUT
        echo "$CHANGELOG_CONTENT" >> $GITHUB_OUTPUT
        echo "EOF" >> $GITHUB_OUTPUT

        # Display changelog for logging
        echo "📋 Generated changelog:"
        echo "========================"
        cat $CHANGELOG_FILE
        echo "========================"

        # Clean up temp file
        rm -f $CHANGELOG_FILE


    - name: Upload test coverage artifact
      uses: actions/upload-artifact@v4
      with:
        name: test-coverage-html-${{ matrix.python-runtime }}
        path: htmlcov/
        retention-days: 30

    - name: Create GitHub Release
      id: create_release
      if: github.event_name == 'push' && github.ref == 'refs/heads/master'
      uses: softprops/action-gh-release@v2
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: v${{ steps.version.outputs.version }}
        name: Release v${{ steps.version.outputs.version }}
        body: |
          ## Alchimist Project - Arbitrage Option Strategies CLI

          **Version:** ${{ steps.version.outputs.version }}
          **Commit:** ${{ steps.version.outputs.git_hash }}

          ${{ steps.changelog.outputs.changelog }}

          ### 🎯 What's New in This Release
          - Automated release with version ${{ steps.version.outputs.version }}
          - Includes all source code and dependencies
          - Excludes virtual environment for clean distribution
          - Comprehensive test coverage with pytest integration tests

          ### 📦 Installation
          1. Download the zip file below
          2. Extract to your desired location
          3. Create a virtual environment: `python -m venv venv`
          4. Activate the environment: `source venv/bin/activate` (Linux/Mac) or `venv\Scripts\activate` (Windows)
          5. Install dependencies: `pip install -r requirements.txt`
          6. Run the CLI: `python alchimest.py --help`

          ### 🧪 Testing
          Run the test suite: `python -m pytest tests/ -v`

          ### 📋 Project Links
          - **Repository:** [alc-lite](https://github.com/${{ github.repository }})
          - **Issues:** [Report a bug](https://github.com/${{ github.repository }}/issues)
          - **Discussions:** [Join the conversation](https://github.com/${{ github.repository }}/discussions)

          ### ⚠️ Disclaimer
          This software is provided for educational and research purposes only. Trading financial instruments involves significant risk of loss. Always do your own research and consider consulting with a qualified financial advisor before making investment decisions.
        draft: false
        prerelease: false

  upload-artifacts:
    needs: ci
    runs-on: macos-latest
    if: success()

    permissions:
      contents: write
      actions: read
      packages: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Get version information
      id: version
      run: |
        # Extract version from Python files or use git tag
        if [ -f "alchimest.py" ]; then
          VERSION=$(grep -o '__version__ = "[^"]*"' alchimest.py | cut -d'"' -f2 || echo "0.1.0")
        else
          VERSION="0.1.0"
        fi

        # Get git information
        GIT_HASH=$(git rev-parse --short HEAD)
        GIT_TAG=$(git describe --tags --exact-match 2>/dev/null || echo "dev")

        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "git_hash=$GIT_HASH" >> $GITHUB_OUTPUT
        echo "git_tag=$GIT_TAG" >> $GITHUB_OUTPUT

        echo "Version: $VERSION"
        echo "Git Hash: $GIT_HASH"
        echo "Git Tag: $GIT_TAG"

    - name: Create zip artifact
      run: |
        # Create a temporary directory for the zip
        mkdir -p temp_zip

        # Copy all files except venv and .git
        rsync -av --exclude='venv/' --exclude='.git/' --exclude='temp_zip/' --exclude='.github/' ./ temp_zip/

        # Create zip file with version info
        cd temp_zip
        zip -r "../alc-lite-v${{ steps.version.outputs.version }}-${{ steps.version.outputs.git_hash }}.zip" .
        cd ..

    - name: Upload zip artifact
      uses: actions/upload-artifact@v4
      with:
        name: alc-lite-v${{ steps.version.outputs.version }}-${{ steps.version.outputs.git_hash }}
        path: alc-lite-v${{ steps.version.outputs.version }}-${{ steps.version.outputs.git_hash }}.zip
        retention-days: 30

    - name: Cleanup
      run: |
        rm -rf temp_zip
        rm -f alc-lite-v${{ steps.version.outputs.version }}-${{ steps.version.outputs.git_hash }}.zip

  pypy-benchmark:
    needs: upload-artifacts
    runs-on: macos-latest
    if: success()

    permissions:
      contents: read
      actions: read

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up PyPy for benchmarking
      uses: actions/setup-python@v5
      with:
        python-version: 'pypy3.10'

    - name: Install PyPy dependencies
      run: |
        python -m pip install --upgrade pip

        # Install PyPy-compatible dependencies
        if [ -f "requirements-pypy.txt" ]; then
          pip install -r requirements-pypy.txt
        else
          pip install -r requirements.txt
        fi

        # Skip development dependencies that may not work with PyPy
        echo "✅ PyPy dependencies installed"

    - name: Run PyPy SFR benchmark
      run: |
        set -e  # Exit on any error
        echo "🚀 Running SFR benchmark with PyPy..."
        echo "Python version: $(python --version)"
        echo "Working directory: $(pwd)"

        cd benchmarks
        echo "Benchmarks directory contents:"
        ls -la

        # Run benchmark and capture any errors
        if ! python sfr_end_to_end_benchmark.py --seed 42 --output pypy_benchmark_results.json; then
          echo "❌ PyPy benchmark failed with exit code $?"
          exit 1
        fi

        # Verify file was created
        if [ ! -f "results/pypy_benchmark_results.json" ]; then
          echo "❌ PyPy benchmark results file not created"
          echo "Results directory contents:"
          ls -la results/ || echo "results directory doesn't exist"
          exit 1
        fi

        echo "✅ PyPy benchmark completed successfully"
        echo "Results directory:"
        ls -la results/

    - name: Set up CPython for comparison
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'

    - name: Install CPython dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        echo "✅ CPython dependencies installed"

    - name: Run CPython SFR benchmark
      run: |
        set -e  # Exit on any error
        echo "🐍 Running SFR benchmark with CPython..."
        echo "Python version: $(python --version)"

        cd benchmarks
        echo "Directory contents before CPython benchmark:"
        ls -la results/ || echo "results directory doesn't exist yet"

        # Run benchmark and capture any errors
        if ! python sfr_end_to_end_benchmark.py --seed 42 --output cpython_benchmark_results.json; then
          echo "❌ CPython benchmark failed with exit code $?"
          exit 1
        fi

        # Verify file was created
        if [ ! -f "results/cpython_benchmark_results.json" ]; then
          echo "❌ CPython benchmark results file not created"
          echo "Results directory contents:"
          ls -la results/ || echo "results directory doesn't exist"
          exit 1
        fi

        echo "✅ CPython benchmark completed successfully"
        echo "Final results directory:"
        ls -la results/

    - name: Generate benchmark comparison
      run: |
        set -e  # Exit on any error
        echo "📊 Generating benchmark comparison..."
        cd benchmarks

        # Verify both files exist before comparison
        echo "Checking for benchmark result files:"
        if [ ! -f "results/pypy_benchmark_results.json" ]; then
          echo "❌ PyPy results file missing"
          exit 1
        fi
        if [ ! -f "results/cpython_benchmark_results.json" ]; then
          echo "❌ CPython results file missing"
          exit 1
        fi
        echo "✅ Both benchmark result files found"
        ls -la results/
        python << 'EOF'
        import json
        import os
        from datetime import datetime

        def load_benchmark_results(filename):
            try:
                with open(f"results/{filename}", 'r') as f:
                    return json.load(f)
            except FileNotFoundError:
                print(f"Warning: {filename} not found")
                return None

        def calculate_speedup(pypy_time, cpython_time):
            if cpython_time > 0:
                return cpython_time / pypy_time
            return 0

        def generate_comparison_report(pypy_results, cpython_results):
            report = ["# 🏎️ PyPy vs CPython Performance Comparison", ""]
            report.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report.append(f"**Commit:** {os.environ.get('GITHUB_SHA', 'unknown')[:8]}")
            report.append("")

            if not pypy_results or not cpython_results:
                report.append("❌ **Error:** Missing benchmark results")
                return "\n".join(report)

            # Add quick summary table
            report.append("## 📊 Quick Summary")
            report.append("")

            pypy_benchmarks = pypy_results.get('benchmarks', {})
            cpython_benchmarks = cpython_results.get('benchmarks', {})

            # Build summary table
            report.append("| Test | PyPy | CPython | Speedup | Status |")
            report.append("|------|------|---------|---------|--------|")

            # Arbitrage detection summary (using 10k options as representative)
            if 'arbitrage_detection' in pypy_benchmarks and 'arbitrage_detection' in cpython_benchmarks:
                pypy_arb = pypy_benchmarks['arbitrage_detection'].get('size_10000', {})
                cpython_arb = cpython_benchmarks['arbitrage_detection'].get('size_10000', {})
                if 'timing' in pypy_arb and 'timing' in cpython_arb:
                    pypy_time = pypy_arb['timing']['mean']
                    cpython_time = cpython_arb['timing']['mean']
                    speedup = calculate_speedup(pypy_time, cpython_time)
                    pypy_throughput = pypy_arb.get('throughput_options_per_sec', 0)
                    cpython_throughput = cpython_arb.get('throughput_options_per_sec', 0)

                    if speedup > 1.1:
                        status = f"✅ {speedup:.1f}x faster"
                    elif speedup > 0.9:
                        status = "⚖️ Similar"
                    else:
                        status = f"🐢 {1/speedup:.1f}x slower"

                    report.append(f"| Arbitrage Detection (10K opts) | {pypy_throughput:.0f} opts/s | {cpython_throughput:.0f} opts/s | {speedup:.2f}x | {status} |")

            # Full scan summary (using 20 symbols as representative)
            if 'full_sfr_scan_simulation' in pypy_benchmarks and 'full_sfr_scan_simulation' in cpython_benchmarks:
                pypy_scan = pypy_benchmarks['full_sfr_scan_simulation'].get('symbols_20', {})
                cpython_scan = cpython_benchmarks['full_sfr_scan_simulation'].get('symbols_20', {})
                if 'timing' in pypy_scan and 'timing' in cpython_scan:
                    pypy_time = pypy_scan['timing']['mean']
                    cpython_time = cpython_scan['timing']['mean']
                    speedup = calculate_speedup(pypy_time, cpython_time)

                    if speedup > 1.1:
                        status = f"✅ {speedup:.1f}x faster"
                    elif speedup > 0.9:
                        status = "⚖️ Similar"
                    else:
                        status = f"🐢 {1/speedup:.1f}x slower"

                    report.append(f"| Full Scan (20 symbols) | {pypy_time:.3f}s | {cpython_time:.3f}s | {speedup:.2f}x | {status} |")

            report.append("")

            # Runtime info
            pypy_runtime = pypy_results.get('runtime_info', {})
            cpython_runtime = cpython_results.get('runtime_info', {})

            report.append("## 🔧 Runtime Information")
            report.append(f"- **PyPy:** {pypy_runtime.get('runtime', 'Unknown')} {pypy_runtime.get('pypy_version', 'Unknown')}")
            report.append(f"- **CPython:** {cpython_runtime.get('runtime', 'Unknown')} {cpython_runtime.get('python_version', 'Unknown').split()[0]}")
            report.append("")

            # Performance summary
            report.append("## 📈 Performance Summary")

            pypy_benchmarks = pypy_results.get('benchmarks', {})
            cpython_benchmarks = cpython_results.get('benchmarks', {})

            # Arbitrage detection comparison (collapsible)
            if 'arbitrage_detection' in pypy_benchmarks and 'arbitrage_detection' in cpython_benchmarks:
                report.append("<details>")
                report.append("<summary>🎯 Detailed Arbitrage Detection Results</summary>")
                report.append("")
                report.append("| Options Count | PyPy Time | CPython Time | PyPy Throughput | CPython Throughput | Speedup |")
                report.append("|---------------|-----------|--------------|-----------------|-------------------|---------|")

                pypy_arb = pypy_benchmarks['arbitrage_detection']
                cpython_arb = cpython_benchmarks['arbitrage_detection']

                for size_key in sorted(pypy_arb.keys()):
                    if size_key in cpython_arb and 'timing' in pypy_arb[size_key] and 'timing' in cpython_arb[size_key]:
                        pypy_time = pypy_arb[size_key]['timing']['mean']
                        cpython_time = cpython_arb[size_key]['timing']['mean']
                        speedup = calculate_speedup(pypy_time, cpython_time)

                        pypy_throughput = pypy_arb[size_key].get('throughput_options_per_sec', 0)
                        cpython_throughput = cpython_arb[size_key].get('throughput_options_per_sec', 0)

                        size = size_key.split('_')[-1]
                        speedup_text = f"{speedup:.2f}x" if speedup >= 1 else f"{1/speedup:.2f}x slower"

                        # Format numbers with commas separately to avoid f-string conflict
                        size_formatted = f"{int(size):,}"
                        pypy_throughput_formatted = f"{pypy_throughput:,.0f}"
                        cpython_throughput_formatted = f"{cpython_throughput:,.0f}"

                        report.append(f"| {size_formatted} | {pypy_time:.4f}s | {cpython_time:.4f}s | {pypy_throughput_formatted} opts/s | {cpython_throughput_formatted} opts/s | {speedup_text} |")

                report.append("")
                report.append("</details>")
                report.append("")

            # Full scan comparison (collapsible)
            if 'full_sfr_scan_simulation' in pypy_benchmarks and 'full_sfr_scan_simulation' in cpython_benchmarks:
                report.append("<details>")
                report.append("<summary>🔍 Detailed Full Scan Performance</summary>")
                report.append("")
                report.append("| Symbol Count | PyPy Time | CPython Time | PyPy Throughput | CPython Throughput | Speedup |")
                report.append("|--------------|-----------|--------------|-----------------|-------------------|---------|")

                pypy_scan = pypy_benchmarks['full_sfr_scan_simulation']
                cpython_scan = cpython_benchmarks['full_sfr_scan_simulation']

                for symbols_key in sorted(pypy_scan.keys()):
                    if symbols_key in cpython_scan and 'timing' in pypy_scan[symbols_key] and 'timing' in cpython_scan[symbols_key]:
                        pypy_time = pypy_scan[symbols_key]['timing']['mean']
                        cpython_time = cpython_scan[symbols_key]['timing']['mean']
                        speedup = calculate_speedup(pypy_time, cpython_time)

                        pypy_throughput = pypy_scan[symbols_key].get('throughput', {}).get('symbols_per_sec', 0)
                        cpython_throughput = cpython_scan[symbols_key].get('throughput', {}).get('symbols_per_sec', 0)

                        count = symbols_key.split('_')[-1]
                        speedup_text = f"{speedup:.2f}x" if speedup >= 1 else f"{1/speedup:.2f}x slower"

                        report.append(f"| {count} | {pypy_time:.4f}s | {cpython_time:.4f}s | {pypy_throughput:.1f} sym/s | {cpython_throughput:.1f} sym/s | {speedup_text} |")

                report.append("")
                report.append("</details>")
                report.append("")

            report.append("")
            report.append("## 🎯 Key Takeaways")

            # Calculate overall performance trend
            speedup_values = []
            if 'arbitrage_detection' in pypy_benchmarks and 'arbitrage_detection' in cpython_benchmarks:
                pypy_arb = pypy_benchmarks['arbitrage_detection']
                cpython_arb = cpython_benchmarks['arbitrage_detection']
                for size_key in pypy_arb:
                    if size_key in cpython_arb and 'timing' in pypy_arb[size_key] and 'timing' in cpython_arb[size_key]:
                        pypy_time = pypy_arb[size_key]['timing']['mean']
                        cpython_time = cpython_arb[size_key]['timing']['mean']
                        speedup = calculate_speedup(pypy_time, cpython_time)
                        if speedup > 0:
                            speedup_values.append(speedup)

            if speedup_values:
                avg_speedup = sum(speedup_values) / len(speedup_values)
                if avg_speedup > 1.1:
                    report.append(f"- ✅ **PyPy shows {avg_speedup:.2f}x average performance improvement**")
                    report.append("- 🚀 PyPy JIT compilation provides significant benefits for computation-heavy tasks")
                elif avg_speedup > 0.9:
                    report.append("- ⚖️ **Performance is comparable between PyPy and CPython**")
                else:
                    report.append(f"- ⚠️ **CPython is {1/avg_speedup:.2f}x faster on average**")
                    report.append("- 💡 PyPy JIT may need more warm-up time or these workloads favor CPython")
            else:
                report.append("- ❓ **Unable to calculate performance comparison**")

            report.append("- 🔄 Results based on identical seed (42) for reproducible comparison")
            report.append("- 📊 Benchmark includes realistic options chain processing and arbitrage detection")

            return "\n".join(report)

        # Load results
        pypy_results = load_benchmark_results("pypy_benchmark_results.json")
        cpython_results = load_benchmark_results("cpython_benchmark_results.json")

        # Generate comparison
        comparison = generate_comparison_report(pypy_results, cpython_results)

        # Save comparison report
        with open("benchmark_comparison.md", "w") as f:
            f.write(comparison)

        print("📄 Benchmark comparison generated")
        print("\n" + comparison)
        EOF

    - name: Upload PyPy benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: pypy-benchmark-results
        path: |
          benchmarks/results/pypy_benchmark_results.json
          benchmarks/results/cpython_benchmark_results.json
          benchmarks/benchmark_comparison.md
        retention-days: 30

    - name: Display benchmark summary
      run: |
        echo "📊 Benchmark Summary:"
        echo "===================="
        if [ -f "benchmarks/benchmark_comparison.md" ]; then
          cat benchmarks/benchmark_comparison.md
        else
          echo "❌ Benchmark comparison not generated"
        fi

    - name: Add benchmark results to job summary
      if: always()  # Run even if previous steps fail
      run: |
        echo "# 🏎️ PyPy vs CPython Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Add timestamp and commit info
        echo "**Run Date:** $(date '+%Y-%m-%d %H:%M:%S')" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Check if comparison file exists and add it
        if [ -f "benchmarks/benchmark_comparison.md" ]; then
          cat benchmarks/benchmark_comparison.md >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Error:** Benchmark comparison was not generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add debug info if benchmarks failed
          echo "## Debug Information" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check individual benchmark files
          if [ -f "benchmarks/results/pypy_benchmark_results.json" ] && [ ! -f "benchmarks/results/cpython_benchmark_results.json" ]; then
            echo "⚠️ **Warning:** Only PyPy benchmark completed" >> $GITHUB_STEP_SUMMARY
          elif [ ! -f "benchmarks/results/pypy_benchmark_results.json" ] && [ -f "benchmarks/results/cpython_benchmark_results.json" ]; then
            echo "⚠️ **Warning:** Only CPython benchmark completed" >> $GITHUB_STEP_SUMMARY
          elif [ ! -f "benchmarks/results/pypy_benchmark_results.json" ] && [ ! -f "benchmarks/results/cpython_benchmark_results.json" ]; then
            echo "❌ **Error:** No benchmark results found" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -d "benchmarks/results" ]; then
            echo "### Files in benchmarks/results:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -la benchmarks/results/ >> $GITHUB_STEP_SUMMARY 2>&1
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Results directory not found" >> $GITHUB_STEP_SUMMARY
          fi
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "---" >> $GITHUB_STEP_SUMMARY
        echo "*View full logs for detailed benchmark output*" >> $GITHUB_STEP_SUMMARY
